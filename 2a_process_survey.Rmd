---
title: "Process West Coast Groundfish Trawl Survey Data"
author: "Brian Stock"
date: "Sept 29, 2017"
output: html_vignette
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc} 
---

This vignette demonstrates how we downloaded and processed the data for:

> Stock BC, Ward EJ, Eguchi T, Jannot JE, Thorson JT, Feist BE, and Semmens BX. "Random forests outperform other species distribution models for spatiotemporal fisheries bycatch prediction."

Because the fisheries observer datasets we used are confidential ([WCGOP](https://www.nwfsc.noaa.gov/research/divisions/fram/observation/data_collection/manuals/2017%20WCGOP%20Training%20Manual%20Final%20website%20copy.pdf), [HILL](http://www.nmfs.noaa.gov/pr/interactions/fkwtrt/meeting1/handouts/observer_manual.pdf)), here we perform the same analyses using the publically available [West Coast Groundfish Trawl Survey](https://www.nwfsc.noaa.gov/research/divisions/fram/groundfish/bottom_trawl.cfm).

If you 

# Want to get data frame where each row is a unique haul, with the following columns:
#   - HAUL_ID: haul ID as in HAUL$Trawl.Id
#   - YEAR: year
#   - DATE: HAUL$Trawl.Date
#   - TIME: HAUL$Trawl.Start.Time
#   - LAT
#   - LONG
#   - DEPTH
#   - EFFORT1: area swept
#   - EFFORT2: haul duration
#   - BOT_TEMP: bottom temperature (at gear)
#   - YEYE: yelloweye rockfish catch weight in kg (species=="Sebastes ruberrimus", 0 for all other hauls)
#   - DBRK: darkblotched rockfish catch weight in kg (species=="Sebastes crameri", 0 for all other hauls)
#   - PHLB: Pacific halibut catch weight in kg (species=="Hippoglossus stenolepis", 0 for all other hauls)
#   - SST: sea-surface temperature - have to get online

library(dplyr)
library(PBSmapping)
library(maps)
library(fields)
library(gplots)
library(sp)
library(INLA)
library(ncdf)
library(date)
library(mvtnorm)
library(ROCR)
library(splancs)
library(MASS)

setwd("/home/brian/Documents/Bycatch/WCGOP/data") 				# linux
HAUL <- read.csv("wcann_haul.csv",header=TRUE)
FISH <- read.csv("wcann_fish.csv",header=TRUE)

# For some reason, the HAUL dataset is duplicated - remove duplicate rows
hauls <- unique(HAUL$Trawl.Id)
n.hauls <- length(hauls)
HAUL <- HAUL[1:n.hauls,]

# Create data frame where each row is a unique haul
#   Center/de-mean the covariates (subtract mean from each observation)
na_vec <- rep(NA,n.hauls)
zero_vec <- rep(0,n.hauls)
dat <- data.frame(HAUL_ID=na_vec,YEAR=na_vec,DATE=as.Date(na_vec),TIME=na_vec,LAT=na_vec,LONG=na_vec,DEPTH=na_vec,EFFORT1=na_vec,EFFORT2=na_vec,BOT_TEMP=na_vec,SST=na_vec,YEYE=zero_vec,DBRK=zero_vec,PHLB=zero_vec)
dat$HAUL_ID <- HAUL$Trawl.Id
dat$DATE <- as.Date(HAUL$Trawl.Date,format = "%m/%d/%y")
dat$YEAR <- as.numeric(format(dat$DATE,"%Y"))
dat$TIME <- as.POSIXct(as.character(HAUL$Trawl.Start.Time), format = "%m/%d/%y %H:%M", tz="US/Pacific")
attr(dat$TIME, "tzone") <- "US/Pacific"
dat$LAT <- HAUL$Best.Latitude..dd.
dat$LONG <- HAUL$Best.Longitude..dd.
dat$DEPTH <- HAUL$Best.Depth..m.
dat$EFFORT1 <- HAUL$Area.Swept.by.the.Net..hectares.
dat$EFFORT2 <- HAUL$Trawl.Duration..min.
dat$BOT_TEMP <- HAUL$Temperature.At.the.Gear..degs.C.
for(i in 1:n.hauls){
	cur_haul <- filter(FISH,Trawl.Id==dat$HAUL_ID[i])
	if("Sebastes ruberrimus" %in% cur_haul$Species) dat$YEYE[i] <- as.numeric(dplyr::filter(cur_haul,Species=="Sebastes ruberrimus") %>% dplyr::select(Haul.Weight..kg.))
	if("Sebastes crameri" %in% cur_haul$Species) dat$DBRK[i] <- as.numeric(dplyr::filter(cur_haul,Species=="Sebastes crameri") %>% dplyr::select(Haul.Weight..kg.))
	if("Hippoglossus stenolepis" %in% cur_haul$Species) dat$PHLB[i] <- as.numeric(dplyr::filter(cur_haul,Species=="Hippoglossus stenolepis") %>% dplyr::select(Haul.Weight..kg.))
}

# Read in "get_SST" function from "process_wcgop.r"
dat <- get_SST(dat)
# delete any records where SST is NA (71 of 6453)
dat <- dat[-which(is.na(dat$SST)),]

# Check for NAs in all dat columns
col.na <- function(vec){ return(length(which(is.na(vec))))} # returns number of NAs in a vector
apply(dat,2,col.na)
# bottom/gear temp has 274 NAs - remove them
dat <- dat[-which(is.na(dat$BOT_TEMP)),]

# get new number of hauls (6108)
n.hauls <- dim(dat)[1]

# Non-zero catch rates
length(which(dat$PHLB!=0))/n.hauls # 0.081
length(which(dat$DBRK!=0))/n.hauls # 0.177
length(which(dat$YEYE!=0))/n.hauls # 0.021

# Add rockdist, rocksize, and inRCA covariates (from Blake)
library(tidyr)
rca <- read.csv("/home/brian/Documents/Bycatch/WCGOP/data/rca_boundaries.csv",header=TRUE)

years <- sort(as.numeric(levels(as.factor(rca$Year))),decreasing=TRUE)
get_n_bins <- function(yr) {a <- rca %>% dplyr::filter(Year==yr) %>% dplyr::select(Lat.low) %>% dim; return(a[1])}
n.bins <- sapply(years,get_n_bins)
LAT.bins <- NULL
for(yr in 1:length(n.bins)){ LAT.bins <- c(LAT.bins,n.bins[yr]:1) }
rca.new <- rca %>% mutate(LAT.bin=LAT.bins) %>% gather(Month,Close,Jan:Dec)
close.lohi <- matrix(as.numeric(unlist(strsplit(rca.new$Close,"-"))), ncol=2, byrow=TRUE)
rca.new <- rca.new %>% mutate(close.low=close.lohi[,1],close.high=close.lohi[,2])

blake <- read.table("survey_points_with_attributes.txt",header=TRUE,sep=",")
checkRCA <- dplyr::filter(blake,Fath_categ!="250+") # only could be in an RCA if depth < 250 fm
checkRCA <- dplyr::filter(checkRCA,Year!=2002) # no RCA closures in 2002

# test <- checkRCA[1:1000,]
blake$Month <- format(as.Date(blake$Date),"%b")
blake$inRCA = 0 # add "inRCA" covariate (0 if not, 1 if yes)
blake$bin = 0
for(j in 1:nrow(checkRCA)){
	i <- checkRCA$Master_id[j]
	breaks <- c(55,rca %>% dplyr::filter(Year==blake$Year[i]) %>% dplyr::select(Lat.low) %>% unlist)
	blake$bin[i] <- cut(blake$Lat[i],breaks=breaks,labels=1:(length(breaks)-1))
	low <- rca.new %>% dplyr::filter(Year==blake$Year[i],Month==blake$Month[i],LAT.bin==blake$bin[i]) %>% dplyr::select(close.low)
	high <- rca.new %>% dplyr::filter(Year==blake$Year[i],Month==blake$Month[i],LAT.bin==blake$bin[i]) %>% dplyr::select(close.high)
	if(abs(blake$Ngdc_fath[i]) < high & abs(blake$Ngdc_fath[i]) > low) blake$inRCA[i] = 1
}
# # Check inRCA is working
# pos <- filter(test,inRCA==1) %>% select(Year,Month,Lat,Ngdc_fath,bin)
# rca.new %>% filter(Year==pos$Year[i],Month==pos$Month[i],LAT.bin==pos$bin[i])

inRCA_summary <- blake %>% group_by(Year) %>% summarise(count=n(),inRCA=sum(inRCA)) %>% mutate(propRCA=inRCA/count)

pdf("/home/brian/Documents/Bycatch/WCGOP/figures/survey_inRCA.pdf")
plot(inRCA_summary$Year,inRCA_summary$inRCA,type='o',xlab="Year",ylab="Number of tows in RCAs")
dev.off()

pdf("/home/brian/Documents/Bycatch/WCGOP/figures/survey_propRCA.pdf")
plot(inRCA_summary$Year,inRCA_summary$propRCA,type='o',xlab="Year",ylab="Percent of tows in RCAs")
dev.off()

# # Make sure blake$LAT == out$LAT etc.
# identical(blake$Lat,dat$LAT)
# identical(blake$Long,dat$LONG)
# identical(blake$Year,dat$YEAR)

# Copy covariates to 'out' and save workspace
dat$inRCA <- blake$inRCA
dat$ROCKDIST <- blake$Rockdist_m
dat$ROCKSIZE <- blake$Rocksizeha

# data are loaded and "cleaned" = all original variables are good to go, no NAs (not transformed, centered)
save.image("survey_cleaned.RData")

# Now "process" the data - transform, center, make ready for model fitting
# Original untransformed, uncentered data in ALL CAPS, e.g. DEPTH
# Transformed, ready to fit data in lower case, e.g. logDEPTH, logDEPTH2
setwd("/home/brian/Documents/Bycatch/WCGOP/data")
load("survey_cleaned.RData")

# Log- or Sqrt-transform covariates on large scales
dat$logDEPTH <- log(dat$DEPTH)
dat$sqrtROCKDIST <- sqrt(dat$ROCKDIST)
dat$logROCKSIZE <- log(dat$ROCKSIZE)
dat$logEFFORT1 <- log(dat$EFFORT1)
dat$logEFFORT2 <- log(dat$EFFORT2)

# Center/de-mean each covariate (for easier model fitting)
dat$sst <- dat$SST # - min(dat$SST)
dat$bot_temp <- dat$BOT_TEMP
demean <- function(vec){ return(vec - mean(vec))}
dat[,c(8,9,18:24)] <- apply(dat[,c(8,9,18:24)],2,demean)

# Create squared covariates
dat$sst2 <- dat$sst^2
dat$logDEPTH2 <- dat$logDEPTH^2
dat$bot_temp2 <- dat$BOT_TEMP^2

# Data are ready to fit
# save(dat,file="survey_processed.RData")
save(dat,file="survey_processed2.RData")

# # Get survey points for Blake to make inRCA covariate
# locations <- dplyr::select(dat,YEAR,DATE,LAT,LONG)
# write.csv(locations,file="survey_points.csv",row.names=FALSE)

# Standard-normalize covariates
# boxcox_getlambda <- function(vec){
# 	vec.lm <<- vec
# 	out <- MASS::boxcox(lm(vec.lm~1),plotit=FALSE)
# 	best <- out$x[which(out$y==max(out$y))]
# 	return(best)
# }
# boxcox_transform <- function(vec,lambda){
# 	out <- (vec^lambda - 1)/lambda
# 	return(out)
# }
# apply(dat[,7:10],2,boxcox_getlambda)
# new2 <- boxcox_transform(dat$EFFORT2,-3)
# new1 <- boxcox_transform(dat$EFFORT1,-1.2)

# tmp <- exp(rnorm(10))
# out <- boxcox(lm(tmp~1))
# range(out$x[out$y > max(out$y)-qchisq(0.95,1)/2])


# pdf("rockdist_untransformed.pdf")
# hist(dat$Rockdist_m)
# dev.off()

# pdf("rockdist_logplus1.pdf")
# hist(log(dat$Rockdist_m + 1))
# dev.off()

# pdf("rockdist_sqrt.pdf")
# hist(sqrt(dat$Rockdist_m))
# dev.off()

# library(dplyr)
# over500 <- dat %>% dplyr::filter(Rockdist_m > 500) %>% select(Rockdist_m)
# over50 <- dat %>% dplyr::filter(Rockdist_m > 50) %>% select(Rockdist_m)
# pdf("rockdist_logcutoff.pdf")
# hist(log(over50[,1]))
# dev.off()

# Add DEPTH2 and SST2 covariates (have to re-mean first)


